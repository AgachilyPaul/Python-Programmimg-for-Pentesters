#导入库文件
import csv
import requests #requests是基于urllib3的一个用于发起http请求的库
import re #用于字符串匹配
import time #用于处理时间

#设置hearder模拟浏览器发出请求
HEADERS = {
    'user-agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'
}


def spider_page(url):
    """
    爬取某一页的数据
    :param url:
    :return:
    """
    response = requests.get(url, headers=HEADERS)
    text_raw = response.text

    # print(text_raw)

    # 1.获取所有的标题
    titles = re.findall(r'<div\sclass="cont">.*?<b>(.*?)</b>', text_raw, re.DOTALL)

    # 2.获取所有的朝代
    dynasties = re.findall(r'<p\sclass="source">.*?<a.*?>(.*?)</a>', text_raw, re.DOTALL)

    # 3.获取作者信息
    authors = re.findall(r'<p\sclass="source">.*?<a.*?>.*?<a.*?>(.*?)</a>', text_raw, re.DOTALL)

    # 4.获取古诗文内容
    contents_pre = re.findall(r'<div\sclass="contson".*?>(.*?)</div>', text_raw, re.DOTALL)
    contents = []

    for content_pre in contents_pre:
        # 4.1 利用sub()函数把内容中的【<.*?>或者换行字符】替换为空
        content = re.sub(r'<.*?>|\n', "", content_pre)
        contents.append(content.strip())

    # 诗词列表数据
    poems = []

    # 5. 使用zip()把四个列表组合在一起
    for value in zip(titles, dynasties, authors, contents):
        # 5.1 自动进行解包放入到变量当中
        title, dynastie, author, content = value

        # 5.2 新建dict，并加入到诗词列表数据中
        poem = {
            'title': title,
            'dynastie': dynastie,
            'author': author,
            'content': content
        }
        poems.append(poem)

    return poems


def spider():
    # 全部诗词列表数据
    poems = []

    # 1.爬取前面10页数据
    for page_num in range(10):
        url = 'https://www.gushiwen.org/default_{}.aspx'.format(page_num + 1)
        print('开始爬取第{}页诗词数据'.format(page_num + 1))
        poems.append(spider_page(url))
        time.sleep(1)

    # 2.显示数据
    i = 1
    for poem in poems:
        for a in poem:
            print(i)
            i = i + 1
            for a, b in a.items():
                print(a + ' : ' + b)
        print("==" * 40)

    # 3.将数据写入文件中
    f = open('G:/data_of_poem.csv', 'w', encoding='utf-8')
    csv_writer = csv.writer(f)
    csv_writer.writerow(["标题", "朝代", "作者"])
    for poem in poems:
        for a in poem:
            write_in = []
            # 先将元组中的内容转化到列表中
            for a, b in a.items():
                write_in.append(b)
            csv_writer.writerow([write_in[0], write_in[1], write_in[2]])
    f.close()

    print('恭喜！爬取数据完成！')


if __name__ == '__main__':
    spider()
